url_data <- readLines(myurl,encoding="UTF-8")
####title추출####
final_filter_data <- url_data[str_detect(url_data,"subject_fixed")]
title <- str_extract(final_filter_data,"(?<=\">).*(?=</span>)")
hit_data <- final_filter_data[str_detect(final_filter_data,"<span class=\"hit\">")]
hit <- str_extract(hit_data,"(?<=\">).*(?=</span>)")[-1]
str_detect(url_data,"subject_fixed")
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val, end= -3)
url_val <- paste0("http://www.clien.net", url_val)
#### url을 이용해서 content항목 추출 ####
#content를 담을 리스트
contentlist=NULL #최초변수 선언 시 null로 초기화
for(page in 1:length(url_val)){
contentdata <- readLines(url_val[page],encoding = "UTF-8")
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
#데이터 정제
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data,collapse="")
content_filter_data <- gsub("<.*?>","",content_filter_data)
content <- gsub("\t|&nbsp;","",content_filter_data)
#기존의 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist,content_filter_data)
cat("\n",page)
}
final_data <- cbind(title,hit,url_val,contentlist)
final_data_list <- rbind(final_data_list,final_data)
cat("\n",i)
}
write.csv(final_data_list,"crawl_data3.csv")
save(final_data_list,file="crawl_data3.RData")
write.csv(final_data_list,"crawl_data5.csv")
save(final_data_list,file="crawl_data5.RData")
con <- mongo(collection = "crawl",
db = "bigdata",
url = "mongodb://127.0.0.1")
#0번부터 9번 페이지까지 반복 작업
final_data_list=NULL
for(i in 0:9){
#페이지마다 연결할 주소가 달라지므로 변수로 처리
myurl <- paste0("https://www.clien.net/service/group/community?$od=T31&po=",i)
url_data <- readLines(myurl,encoding="UTF-8")
####title추출####
final_filter_data <- url_data[str_detect(url_data,"subject_fixed")]
title <- str_extract(final_filter_data,"(?<=\">).*(?=</span>)")
hit_data <- final_filter_data[str_detect(final_filter_data,"<span class=\"hit\">")]
hit <- str_extract(hit_data,"(?<=\">).*(?=</span>)")[-1]
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val, end= -3)
url_val <- paste0("http://www.clien.net", url_val)
#### url을 이용해서 content항목 추출 ####
#content를 담을 리스트
contentlist=NULL #최초변수 선언 시 null로 초기화
content_filter_data=NULL
for(page in 1:length(url_val)){
contentdata <- readLines(url_val[page],encoding = "UTF-8")
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
#데이터 정제
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data,collapse="")
content_filter_data <- gsub("<.*?>","",content_filter_data)
content_filter_data <- gsub("\t|&nbsp;","",content_filter_data)
#기존의 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist,content_filter_data)
cat("\n",page)
}
final_data <- cbind(title,hit,url_val,contentlist)
final_data_list <- rbind(final_data_list,final_data)
cat("\n",i)
}
write.csv(final_data_list,"crawl_data5.csv")
save(final_data_list,file="crawl_data5.RData")
###########mongodb에 저장하기 ############
if(con$count()>0){
con$drop()
}
####mongodb에 데이터를 저장하기 위해서 dataframe으로 변환####
final_mongo_data <- data.frame(final_data_list)
con$insert(final_mongo_data)
install.packages("N2H4")
library(N2H4)
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
library(stringr)
library(dplyr)
getAllComment(url) %>%
select(username, contents)
getAllComment(url)
getAllComment(url) %>%
select(userName, contents) -> mydata
mydata
mycomment <- mydata$contents
mycomment
#css의 선택자를 활용할 수 있는 방법 - rvest
install.packages("rvest")
install.packages("rvest")
library(rvest)
url <- "https://www.clien.net/service/group/community?$od=T31&po=0"
readPage %>%
html_nodes("span.subject_fixed") %>%
#이런 이름으로 정의되어있는 모든 태그들을 뽑아온다.
html_text() -> title_data
title_data
url <- "https://www.clien.net/service/group/community?$od=T31&po=0"
readPage <- read_html(url)
readPage %>%
html_nodes("span.subject_fixed") %>%
#이런 이름으로 정의되어있는 모든 태그들을 뽑아온다.
html_text() -> title_data
title_data
install.packages("KoNLP")
install.packages("KoNLP")
library(KoNLP)
install.packages("Sejong")
library(KoNLP)
install.packages("RSQLite")
install.packages("KoNLP")
install.packages("Sejong")
install.packages("hash")
install.packages("rJava")
install.packages("tau")
install.packages("RSQLite")
install.packages("devtools")
library(KoNLP)
install.packages("Sejong")
install.packages("Sejong")
library(KoNLP)
useSejongDic()
install.packages("KoNLP")
install.packages("Sejong")
install.packages("hash")
install.packages("rJava")
install.packages("tau")
install.packages("RSQLite")
install.packages("devtools")
library(KoNLP)
library(stringr)
######KoNLP의 함수를 테스트######
# 명사만 추출하는 함수
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
#### 분석할 샘플데이터 로딩
load("comments.RData")
load("score.RData")
length()
length(comments)
legend(score)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
data_list = list() #댓글을 분리하ㅣ면 분리된 명사의 갯수가 다르므로, 리스트를 이용
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[2]]
data_list[[22]]
head(data_list,20)
sampledata <- comments[1:1000]
head(comments,10)
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,3,3,3,3,3)
), #반복작업할 데이터 data.frame(test=c(1,2,3,4,5,6),test2=c(3,3,3,3,3,3))
function(x){
x[1]
#반복해서 적용할 함수
}
)
wordlist <- sapply(str_split(data_list,"/"),function(x){
x[1]
})
wordlist
######KoNLP의 함수를 테스트######
# 명사만 추출하는 함수
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
length(comments)
length(score)
length(comments)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,3,3,3,3,3)
), #반복작업할 데이터 data.frame(test=c(1,2,3,4,5,6),test2=c(3,3,3,3,3,3))
function(x){
x[1]
#반복해서 적용할 함수
}
)
wordlist <- sapply(str_split(data_list,"/"),function(x){
x[1]
})
class(wordlist)
head(wordlist,10)
data_list = list() #댓글을 분리하ㅣ면 분리된 명사의 갯수가 다르므로, 리스트를 이용
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
head(data_list,20)
wordlist <- sapply(str_split(data_list,"/"),function(x){
x[1]
})
class(wordlist)
head(wordlist,10)
convertHangulStringToKeyStrokes("R는 많은 공헌자에의한 공동 프로젝트입니다")
library(KoNLP)
library(stringr)
convertHangulStringToKeyStrokes("R는 많은 공헌자에의한 공동 프로젝트입니다")
head(data_list,20)
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
head(data_list,20)
#### 형태소 분석을 하기위해서 명사 분리 ####
data_list = list() #댓글을 분리하ㅣ면 분리된 명사의 갯수가 다르므로, 리스트를 이용
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
head(data_list,20)
#리스트를
class(unlist(data_list))
wordlist <- sapply(str_split(unlist(data_list),"/"), function(x){x[1]})
class(wordlist)
head(wordlist,20)
#table함수를 이용해서 단어의 빈도수를 구하기
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist,descreasing = T)[1:100]
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result)
tablewordlist
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist
tablewordlist_result
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist_result
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist_result
tablewordlist_result
#table함수를 이용해서 단어의 빈도수를 구하기
#table함수는 벡터에 저장되어 있는 모든 단어들의 빈도수를 계산해서 변환
# -> 단어를 이용해서 변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist_result
ins
install.packages("wordcloud")
install.packages("RColorBrewer")
install.packages("wordcloud")
library(RColorBrewer)
library(wordcloud)
#### 분석할 샘플데이터 로딩
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
#### 형태소 분석을 하기위해서 명사 분리 ####
data_list = list() #댓글을 분리하ㅣ면 분리된 명사의 갯수가 다르므로, 리스트를 이용
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
head(data_list,20)
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을  /로 분할
#           => N이 있는 문자열의 첫번째 요소
## sapply를 이용하면 반복작업을 할 수 있다.
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,3,3,3,3,3)
), #반복작업할 데이터 data.frame(test=c(1,2,3,4,5,6),test2=c(3,3,3,3,3,3))
function(x){
x[1]
#반복해서 적용할 함수
}
)
#리스트를
class(unlist(data_list))
wordlist <- sapply(str_split(unlist(data_list),"/"), function(x){x[1]})
wordlist <- sapply(str_split(data_list,"/"),function(x){
x[1]
})
class(wordlist)
head(wordlist,20)
head(data_list,20)
#table함수를 이용해서 단어의 빈도수를 구하기
#table함수는 벡터에 저장되어 있는 모든 단어들의 빈도수를 계산해서 변환
# -> 단어를 이용해서 변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist_result
#table함수는 벡터에 저장되어 있는 모든 단어들의 빈도수를 계산해서 변환
# -> 단어를 이용해서 변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist_result
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,descreasing = T) [1:100]
tablewordlist_result
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T) [1:100]
tablewordlist_result
#table함수를 이용해서 단어의 빈도수를 구하기
#table함수는 벡터에 저장되어 있는 모든 단어들의 빈도수를 계산해서 변환
# -> 단어를 이용해서 변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,descreasing = T) [1:100]
#분석결과를 가지고 기준을 적용해서 정리
# -> 글자 수 중에서 1글자만 있는 것은 제외하기
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T) [1:100]
tablewordlist_result
#RColorBrewer
display.brewer.all()
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,decreasing = T) [1:100]
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T) [1:100]
tablewordlist_result
#RColorBrewer
display.brewer.all()
data_list = list() #댓글을 분리하ㅣ면 분리된 명사의 갯수가 다르므로, 리스트를 이용
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
head(data_list,20)
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,3,3,3,3,3)
), #반복작업할 데이터 data.frame(test=c(1,2,3,4,5,6),test2=c(3,3,3,3,3,3))
function(x){
x[1]
#반복해서 적용할 함수
}
)
class(unlist(data_list))
wordlist <- sapply(str_split(unlist(data_list),"/"), function(x){x[1]})
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,decreasing = T) [1:100]
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T) [1:100]
tablewordlist_result
#RColorBrewer
display.brewer.all(n=10, exact.n = F)
brewer.pal.info
display.brewer.all(n=10, exact.n = F)
brewer.pal.info
#RColorBrewer
#모든 색상 팔레트를 보여준다.
display.brewer.all(n=10, exact.n = F)
brewer.pal.info
display.brewer.all(type="div")
display.brewer.all(type="qual")
display.brewer.all(type="seq")
display.brewer.all(type="qual")
display.brewer.all(type="div")
display.brewer.all(type="qual")
display.brewer.all(type="div")
#분석한 결과가 저장되어 있는 tablewordlist_result의 값을 단어와 숫자로 각각 저장
word <- names(tablewordlist_result)
#분석한 결과가 저장되어 있는 tablewordlist_result의 값을 단어와 숫자로 각각 저장
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
display.brewer.all(type="div")
display.brewer.all(type="qual")
display.brewer.all(type="seq")
display.brewer.all(type="qual")
display.brewer.all(type="qual")
display.brewer.all(type="div")
mycolor <- brewer.pal(n=11,name=PRGn)
mycolor <- brewer.pal(n=11,name="PRGn")
mycolor <- brewer.pal(n=11,name="Spectral")
#분석한 결과가 저장되어 있는 tablewordlist_result의 값을 단어와 숫자로 각각 저장
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
mycolor <- brewer.pal(n=11,name="BrBG")
wordcloud(words=word, freq=count,random.order=F,colors=mycolor,scale=c(7,0.3))
#random.order=F : 빈도수 높은 거 가운데에 놓기
display.brewer.all(type="qual")
display.brewer.all(type="seq")
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
mycolor <- brewer.pal(n=9,name="RdPu")
wordcloud(words=word, freq=count,random.order=F,colors=mycolor,scale=c(7,0.3))
wordcloud(words=word, freq=count,random.order=F,colors=mycolor,scale=c(4,0.3))
display.brewer.all(type="qual")
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
mycolor <- brewer.pal(n=12,name="Set3")
wordcloud(words=word, freq=count,random.order=F,colors=mycolor,scale=c(4,0.3))
display.brewer.all(type="div")
#분석한 결과가 저장되어 있는 tablewordlist_result의 값을 단어와 숫자로 각각 저장
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
mycolor <- brewer.pal(n=11,name="Spectral")
wordcloud(words=word, freq=count,random.order=F,colors=mycolor,scale=c(4,0.3))
#random.order=F : 빈도수 높은 거 가운데에 놓기
tablewordlist_result
brewer.pal.info
