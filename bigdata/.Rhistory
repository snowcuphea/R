install.packages("stringr")
library("stringr")
#패턴
mytext <- "test$uuuu"
install.packages("stringr")
install.packages("stringr")
library("stringr")
#패턴
mytext <- "test$uuuu"
mytext2 <-"https://cran.r-project.org/"
mytext <- "test$uuuu"
mytext2 <-"https://cran.r-project.org/"
#// 패턴과 일치하는 문자열도 리턴
str_extract(mytext,".+(:)") #괄호는 그냥 하나로 묶어주는기능. '.'과 `:`
#패턴
mytext <- "test$uuuuuu"
mytext2 <-"https://cran.r-project.org/"
#// 패턴과 일치하는 문자열도 리턴
str_extract(mytext,".+(:)") #괄호는 그냥 하나로 묶어주는기능.
str_extract(mytext2,".+(?=:")")
#// 패턴과 일치하는 문자열도 리턴
str_extract(mytext2,".+(:)") #괄호는 그냥 하나로 묶어주는기능.
str_extract(mytext2,".+(:)") #괄호는 그냥 하나로 묶어주는기능.
#패턴과 일치하는 문자를 찾아 일치하는 문자 바로 전까지 리턴
str_extract(mytext2,".+(?=:")")
#패턴과 일치하는 문자를 찾아 일치하는 문자 바로 전까지 리턴
str_extract(mytext2,".+(?=:)")
#패턴과 일치하는 문자를 찾아 일치하는 문자 바로 전까지 리턴
str_extract(mytext2,".+(?=:)")
str_extract(mytext,".+(?=$)")
str_extract(mytext,".+(?=\\$)")
str_extract(mytext,"(?<=\\$).*")#후방탐색(?<=)
str <- c("java","hadoop","R","mongodb")
paste(str,collapse=" ")
paste0(mytext, mytext2)
#패턴
mytext <- "   test$uuuuuu"
gsub("u","",mytext)
data <- gsub("u","",mytext)
data
data <- gsub("u","U",mytext)
data
str_trim(data)
install.packages("mongolite")
install.packages("mongolite")
library("stringr")
library("mongolite")
url_data <- readLines(url,encoding="UTF-8")
url_data
url_data <- readLines(url,encoding="UTF-8")
url_data <- readLines(url,encoding="UTF-8")
library("mongolite")
url_data <- readLines(url,encoding="UTF-8")
url <- "https://www.clien.net/service/group/community?$od=T31&po=0"
source('C:/iot/work/RWork/bigdata/basic_crawling.R', echo=TRUE)
url_data <- readLines(url,encoding="UTF-8")
url <- "https://www.clien.net/service/group/community?$od=T31&po=0"
url_data <- readLines(url,encoding="UTF-8")
url_data
#class(url_data)
length(url_data)
class(url_data)
#class(url_data) #타입확인용
#length(url_data) #길이확인용
head(url_data)
tail(url_data)
url_data[200]
#조건에 만족하는 데이터를 필터링
#문자열에 패턴을 적용해서 일치여부를 T/F로 리턴
#str_detect(패턴을 검사할 문자열,패턴)
str_detect(url_data,"subject_fixed")
url_data[str_detect(url_data,"subject_fixed")]
# 2. 추출한 데이터 전체에서 내가 필요한 문자열만 추출
#str_extract -> 패턴에 일치한 데이터를 문자열을 리턴
#전방탐색, 후방탐색을 함께 사용
#str_extract(filter_data,"(?<=).*(?=)")
str_extract(filter_data,"(?<=\">).*(?=</span)")
# 2. 추출한 데이터 전체에서 내가 필요한 문자열만 추출
#str_extract -> 패턴에 일치한 데이터를 문자열을 리턴
#전방탐색, 후방탐색을 함께 사용
#str_extract(filter_data,"(?<=).*(?=)")
str_extract(filter_data,"(?<=\">).*(?=</span>)")
#### 데이터 필터링 : title ####
# 1. str_detect(패턴을 검사할 문자열,패턴)를 이용해서 웹페이지 전체에서 필요한 데이터만 먼저 추출
filter_data <- url_data[str_detect(url_data,"subject_fixed")]
# 2. 추출한 데이터 전체에서 내가 필요한 문자열만 추출
#str_extract -> 패턴에 일치한 데이터를 문자열을 리턴
#전방탐색, 후방탐색을 함께 사용
#str_extract(filter_data,"(?<=).*(?=)")
str_extract(filter_data,"(?<=\">).*(?=</span>)")
# 2. 추출한 데이터 전체에서 내가 필요한 문자열만 추출
#str_extract -> 패턴에 일치한 데이터를 문자열을 리턴
#전방탐색, 후방탐색을 함께 사용
#str_extract(filter_data,"(?<=).*(?=)")
title <- str_extract(filter_data,"(?<=\">).*(?=</span>)")
title
<span class="hit">102</span>
#### 데이터 필터링 : hit 조회수 ####
filter_data <- url_data[str_detect(url_data,"subject_fixed")]
<span class="hit">102</span>
#### 데이터 필터링 : hit 조회수 ####
# 조회수 는 모두 같은 패턴이므로,그냥 처음부터 태그로 써준다.
# `"`를 인식시킬 수 있게, 앞에 \ 를 적어준다.
hit_data <- url_data[str_detect(url_data,"<span class=\"hit\">")]
hit_data
hit <- str_extract(filter_data,"(?<=\">).*(?=</span>)")
hit
hit <- str_extract(hit_data,"(?<=\">).*(?=</span>)")
hit
str_detect(url_data,"subject_fixed")
which(str_detect(url_data,"subject_fixed"))
which(str_detect(url_data,"subject_fixed")+2)
which(str_detect(url_data,"subject_fixed"))+1
which(str_detect(url_data,"subject_fixed"))-2
which(str_detect(url_data,"subject_fixed"))-3
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
myurl
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
myurl
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
source('C:/iot/work/RWork/bigdata/basic_crawling.R', encoding = 'UTF-8', echo=TRUE)
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
myurl
myurl
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
url_val
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
url_val
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
myurl
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
url_val
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
url_val
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
myurl
url_val <- str_extract(myurl,"(?<=href=\").*(?=<data-role)")
url_val
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val
url_val <- paste0("http://www.clien.net", url_val)
#필요없는 문자열을 잘라내기 : end = 3 : 뒤에서 3개를 잘라내기
url_val <- str_sub(url_val, end= -3)
url_val
####csv파일로 생성####
final_data <- cbind(title, hit, url_val)
final_data
write.csv("crawl_data.csv")
write.csv(final_data,"crawl_data.csv")
length(title)
length(hit)
length(url_val)
